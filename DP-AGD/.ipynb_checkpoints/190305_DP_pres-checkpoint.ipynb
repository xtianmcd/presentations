{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\"> Concentrated Differentially Private Gradient Descent with Adaptive per-Iteration Privacy Budget </p>\n",
    "## <p style=\"text-align: center;\"> Jaewoo Lee & Daniel Kifer </p>\n",
    "## <p style=\"text-align: center;\"> KDD 2018 </p>\n",
    "\n",
    "\n",
    "#### Presented by Christian McDaniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\"> Introduction </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Iterative optimization algorithm | Differentially Private iterative algorithms \n",
    ":-------:|:------:\n",
    "<p style=\"text-align: left ;\"> + min f by finding optimal paramter vector w* <br><br>1) Initialize w_0 <br>2) Generate {w_t} s.t. w_t tends toward optimal w as t --> infinity <br><br>e.g., Gradient Descent <br> ![](iter-algo.png) | <p style=\"text-align: left ;\"> Gradient Descent under Differential Privacy <br> ![](gd-dp.png) <br><br> Prior efforts: Naive conversion <br> 1) Pick T a priori <br>2) Split the budget evenly ![](naive-budget.png) <br>3) Update parameters using a noisy graident\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### Naive Privacy conversion has a few issues: \n",
    "\n",
    "+ Accuracy depends on T\n",
    "    + Too Small v. Too Large\n",
    "+ The nature of the gradient changes as the optimum is approached\n",
    "    + Early v. Near Optimum\n",
    "     ![](descent_mag.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### Naive Privacy conversion has a few issues: \n",
    "\n",
    "+ Accuracy depends on T\n",
    "    + Too Small v. Too Large\n",
    "+ The nature of the gradient changes as the optimum is approached\n",
    "    + Early v. Near\n",
    "\n",
    "### Authors Propose:\n",
    "+ (First known) Adaptive Privacy Budget for zero-mean Concentrated Differential Privacy (more later) \n",
    "    + re: Larger v. Smaller norm gradients\n",
    "    + Allows an adaptive T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "### Naive Privacy conversion has a few issues: \n",
    "\n",
    "+ Accuracy depends on T\n",
    "    + Too Small v Too Large\n",
    "+ The nature of the gradient changes as the optimum is approached\n",
    "    + Early v. Near\n",
    "\n",
    "### Authors Propose:\n",
    "+ Adaptive Privacy Budget for zCDP\n",
    "    + At each iteration:\n",
    "        + Partial allocation to calculating the noisy gradient: \n",
    "    <img src=\"noisy-grad.png\" alt=\"ng\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Authors Propose:\n",
    "+ Adaptive Privacy Budget for zCDP\n",
    "    + At each iteration:\n",
    "        + Partial allocation to calculating the noisy gradient\n",
    "        + Allocate the rest to determining the step size:\n",
    "            + Minimize <img src=\"obj-step.png\" alt=\"objs\" width=\"200\" /> \n",
    "            + Update accordingly: <img src=\"w-update.png\" alt=\"wu\" width=\"300\" />\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Differential Privacy \n",
    "\n",
    "![](DP.png)\n",
    "+ When delta=0, said to be pure-DP\n",
    "\n",
    "![](Sens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Gaussian Mechanism\n",
    "\n",
    "![](Gauss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Concentrated DP ![](p-zCDP.png)\n",
    "+ for output o in range(M), the privacy loss random variable Z of M is\n",
    "![](prov-loss.png)\n",
    "+ p-zCDP imposes a bound on the moment generating function of Z and requires it to be bound around 0\n",
    "+ Formally it satisfies: \n",
    "![](renyi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Concentrated DP Lemmas\n",
    "\n",
    "+ Composition\n",
    "\n",
    "![](lemma35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Concentrated DP Lemmas\n",
    "\n",
    "+ zCDP Gaussian Mechanism\n",
    "\n",
    "![](lemma36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Concentrated DP Lemmas\n",
    "\n",
    "+ zCDP - pure-DP conversion\n",
    "\n",
    "![](lemma37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### Concentrated DP Lemmas\n",
    "\n",
    "+ zCDP - (e,d)-DP\n",
    "\n",
    "![](lemma38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "\n",
    "### NoisyMax\n",
    "\n",
    "![](noisymax.png)\n",
    "\n",
    "#### NoisyMin = NoisyMax on - f(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NoisyMax\n",
    "\n",
    "### Implementation ![](phi.png)\n",
    "\n",
    "+ Initialize Phi with m equally-spaced pts b/t 0 and a_max \n",
    "\n",
    "+ Proposed to update a_max in Phi every r iters\n",
    "![](a-phi.png)\n",
    "\n",
    "+ Set m=20, r=10, n_=0.1,a_max=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "## <p style=\"text-align: center;\"> DP-AGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "### Three Main Parts: \n",
    "\n",
    "1. Calculate the Noisy Gradient\n",
    "2. Determine the best step size\n",
    "3. Adjust the privacy budget (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "+ Consider ERM \n",
    "\n",
    "![](erm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "![](algo-inputs.png)\n",
    "\n",
    "+ p_nmax: privacy budget in terms of p-zCDP\n",
    "+ p_ng: initial privacy budget for the gradient approximation \n",
    "+ E_tot, delta_tot: the budget parameters re (E,delta)-DP\n",
    "+ gamma: budget increase rate\n",
    "+ C_obj and C_grad: clipping thresholds for gradient calc and obj fnxn\n",
    "+ {d_1,...,d_n}: set of n observations where each d_i = (x_i, y_i)\n",
    "+ objective function: ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "![](algo-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "![](algo-2.png)\n",
    "\n",
    "![](algo-3.png)\n",
    "\n",
    "(Lemma 3.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "![](algo-4.png)\n",
    "\n",
    "* Checking use of privacy budget \n",
    "\n",
    "### Three Main Parts: \n",
    "\n",
    "1. Calculate the Noisy Gradient\n",
    "2. Determine the best step size\n",
    "3. Adjust the privacy budget (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "1) Private Gradient Approximation\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-5.png)| <p style=\"text-align: left;\"> + Gradient Clipping technique (instead of assuming norm(x) <= 1)<br><br>+ variance of noise depends on sensitivity of g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "1) Private Gradient Approximation\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-6.png) | <p style=\"text-align: left;\"> + Add the noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "1) Private Gradient Approximation\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-7.png) | <p style=\"text-align: left;\"> + Adjust the budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "1) Private Gradient Approximation\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-8.png)| <p style=\"text-align: left;\"> + Normalize the gradient to unit norm (Lemma 3.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "2) Step Size Selection\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-9.png)| <p style=\"text-align: left;\"> + Find optimal step size <br><br>+ NoisyMax(set of candidates, sensitivity, privacy budget for pure DP - Lemma 3.7)<br><br>+ Control the variance of the updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "2) Step Size Selection\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "<img src=\"algo-10.png\" alt=\"sss1\" width=\"400\" /> | <p style=\"text-align: left;\"> Step 14: Check use of privacy budget before updating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "3) Adoptive Noise Reduction - Gradient Averaging\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "![](algo-gavg.png)<br>![](algo-fnxn.png)| <p style=\"text-align: left;\"> + Increase p_t to p_t+1 <br><br>+ Only use p_t+1 - p_t<br><Br>+ Average the gradient from steps t and t+1 to only use a total of p_t+1 across both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "<img src=\"algo-11.png\" alt=\"sss1\" width=\"400\" /> | <p style=\"text-align: left;\">+ loop, dynamically computing budget p (lines 19, 11, 7)<br><br>+ thanks to composition propterties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "_ | _\n",
    ":---:|:---:\n",
    "<img src=\"algo-12.png\" alt=\"sss1\" width=\"400\" />  | <p style=\"text-align: left;\">+ only the weights are visible outside the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def agd_rho(X, y, rho, eps_total, delta, grad_func, loss_func, test_func,\n",
    "            obj_clip, grad_clip, reg_coeff=0.0, batch_size=-1, exp_dec=1.0,\n",
    "            gamma=0.1, splits=60, verbose=False):\n",
    "    N, dim = X.shape\n",
    "\n",
    "    # parameters\n",
    "    eps_nmax = (eps_total * 0.5) / splits\n",
    "    sigma = compute_sigma(eps_nmax, delta, grad_clip)\n",
    "\n",
    "    # intial privacy budget\n",
    "    rho_nmax = 0.5 * (eps_nmax**2)\n",
    "    rho_ng = (grad_clip**2) / (2.0 * sigma**2)\n",
    "    # rho_ng = rho_nmax\n",
    "\n",
    "    # initialize the parameter vector\n",
    "    w = np.zeros(dim)\n",
    "    t = 0\n",
    "    chosen_step_sizes = []\n",
    "    max_step_size = 2.0\n",
    "    n_candidate = 20\n",
    "```                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def agd_rho():\n",
    "    ...\n",
    "while rho > 0:\n",
    "        if verbose:\n",
    "            loss = loss_func(w, X, y) / N\n",
    "            acc = test_func(w, X, y)\n",
    "            print \"[{}] loss: {:.5f}  acc: {:5.2f}\".format(t, loss, acc*100)\n",
    "        if batch_size > 0:\n",
    "            # build a mini-batch\n",
    "            rand_idx = np.random.choice(N, size=batch_size, replace=False)\n",
    "            mini_X = X[rand_idx, :]\n",
    "            mini_y = y[rand_idx]\n",
    "        else:\n",
    "            mini_X = X\n",
    "            mini_y = y\n",
    "                \n",
    "        # non-private (clipped) gradient\n",
    "        grad = grad_func(w, mini_X, mini_y, grad_clip)\n",
    "\n",
    "        sigma = grad_clip / math.sqrt(2.0 * rho_ng)\n",
    "        noisy_grad = grad + sigma * np.random.randn(dim)\n",
    "        noisy_unnorm = np.copy(noisy_grad)\n",
    "        noisy_grad /= np.linalg.norm(noisy_grad)\n",
    "\n",
    "        rho -= rho_ng\n",
    "\n",
    "        # regularization\n",
    "        if reg_coeff > 0:\n",
    "            noisy_grad += reg_coeff * w\n",
    "        idx = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def agd_rho():\n",
    "    ...\n",
    "    while rho > 0:\n",
    "        ...\n",
    "        while idx == 0:\n",
    "                    # test if this is a descent direction\n",
    "                    step_sizes = np.linspace(0, max_step_size, n_candidate+1)\n",
    "                    candidate = [w - step * noisy_grad for step in step_sizes]\n",
    "                    scores = [loss_func(theta, mini_X, mini_y, clip=obj_clip)\n",
    "                              for theta in candidate]\n",
    "                    scores[0] *= exp_dec\n",
    "\n",
    "                    # deduct the privacy budget used for noisy max\n",
    "                    lmbda = obj_clip/math.sqrt(2.0 * rho_nmax)\n",
    "                    idx, _ = noisy_max(scores, lmbda, bmin=True)\n",
    "                    rho -= rho_nmax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def agd_rho():\n",
    "    ...\n",
    "    while rho > 0:\n",
    "        ...\n",
    "        while idx == 0:\n",
    "            ...\n",
    "            # used up the budget\n",
    "            if rho < 0:\n",
    "                break\n",
    "\n",
    "            if idx > 0:\n",
    "                # don't do the update when the remain budget is insufficient\n",
    "                if rho >= 0:\n",
    "                    w[:] = candidate[idx-1]\n",
    "                rho -= rho_ng\n",
    "            else:\n",
    "                rho_old = rho_ng\n",
    "                rho_ng *= (1.0 + gamma)\n",
    "                # max_step_size *= 0.9\n",
    "                if verbose:\n",
    "                    print \"\\tbad gradient: resample (rho_ng={})\".format(rho_ng)\n",
    "\n",
    "                noisy_grad = grad_avg(rho_old, rho_ng, grad, noisy_unnorm,\n",
    "                                      grad_clip)\n",
    "                noisy_grad /= np.linalg.norm(noisy_grad)\n",
    "                rho -= (rho_ng - rho_old)\n",
    "\n",
    "                if reg_coeff > 0:\n",
    "                    noisy_grad += reg_coeff * w\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def agd_rho():\n",
    "    ...\n",
    "    while rho > 0:\n",
    "        ...\n",
    "        while idx == 0:\n",
    "            ...\n",
    "            chosen_step_sizes.append(step_sizes[idx])\n",
    "            t += 1\n",
    "\n",
    "            if (t % 10) == 0:\n",
    "                max_step_size = min(1.1*max(chosen_step_sizes), 2.0)\n",
    "                del chosen_step_sizes[:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` python\n",
    "def grad_avg(rho_old, rho_H, true_grad, noisy_grad, grad_clip):\n",
    "    dim = true_grad.shape[0]\n",
    "    sigma = grad_clip / math.sqrt(2.0 * (rho_H - rho_old))\n",
    "\n",
    "    # new estimate\n",
    "    g_2 = true_grad + sigma * np.random.randn(dim)\n",
    "\n",
    "    beta = rho_old / rho_H\n",
    "    # weighted average\n",
    "    s_tilde = beta * noisy_grad + (1.0 - beta) * g_2\n",
    "\n",
    "    return s_tilde\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results \n",
    "\n",
    "### Datasets\n",
    "\n",
    "![](Tbl2.png)\n",
    "\n",
    "+ delta set to 10^-8 for all except KDDCup99 (10^-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results \n",
    "\n",
    "### Baselines: \n",
    "\n",
    "`ObjPert`: Objective Perturbation, add linear perturbation term to the objective function and solve using non-private solver\n",
    "\n",
    "`OutPert`: Output Perturbation, perturb the output after batch GD for predetermined steps\n",
    "\n",
    "`PrivGene`: DP genetic algorithm\n",
    "\n",
    "`SGD-Adv`: DP algorithm using advanced composition wwith privacy amplification output (upper bound)\n",
    "\n",
    "`SGD-MA`: Uses an improved composition method called moments accountant\n",
    "\n",
    "`NonPrivate`: Non-private L-BFGS algorithm\n",
    "\n",
    "`Majority`: Chooses majority class\n",
    "\n",
    "+ All known DP algorithms use a predetermined privacy budget sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Experiment Design\n",
    "\n",
    "+ Report the classification accuracy and final objective value\n",
    "+ Perform 20 repeats of 5-fold CV\n",
    "\n",
    "#### Preprocessing: \n",
    "+ one-hot encoding of all binary variables\n",
    "+ rescale numberical variables on [0,1]\n",
    "+ for ObjPert, normalie to unit norm (per requirements) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Effects of Parameters\n",
    "\n",
    "+ Ran DP-AGD on Adult dataset\n",
    "+ Overall: robust to parameters - moderate settings have small effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Effects of Parameters (DP-AGD on Adult)\n",
    "\n",
    "+ Splits (T) : Estimate initial privacy budget via number of splits <img src=\"e-nmax.png\" alt=\"enmax\" width=\"200\" /> \n",
    "\n",
    "<img src=\"splits-fx.png\" alt=\"splits\" width=\"600\" />\n",
    "\n",
    "+ Less effect when budget is large \n",
    "+ Too small v Too large \n",
    "+ Splits initialized to 60 for subsequent experiemnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Effects of Parameters (DP-AGD on Adult)\n",
    "\n",
    "+ Gamma\n",
    "\n",
    "<img src=\"gamma-fx.png\" alt=\"sss1\" width=\"700\" /> \n",
    "\n",
    "+ gamma is the amount of change added to the stepwise budget when the noisy gradient is likely not a descent direction (i=0 from NoisyMax)\n",
    "+ gamma > 0.2: no impact on accuracy\n",
    "+ gamma = 0.1 or 0.2: slight impact\n",
    "    + too small a change will require more repeats to find a good direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Effects of Parameters (DP-AGD on Adult)\n",
    "\n",
    "+ C_grad\n",
    "\n",
    "<img src=\"Cgrad-fx.png\" alt=\"sss1\" width=\"700\" /> \n",
    "\n",
    "+ C_x are the clipping parameters : sets a bound on the sensitivity to satisfy zCDP\n",
    "+ C_grad refers to the gradient calculation\n",
    "+ Too small: low sensitivity, but can cause too much information loss\n",
    "+ Too large: high sensitivity, results in too much noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Effects of Parameters (DP-AGD on Adult)\n",
    "\n",
    "+ C_obj\n",
    "\n",
    "<img src=\"Cobj-fx.png\" alt=\"cobj\" width=\"800\" />\n",
    "\n",
    "+ emphasis on moderate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### Logistic Regression\n",
    "<img src=\"logist-reg.png\" alt=\"sss1\" height=\"10\" width=\"700\" /> \n",
    "\n",
    "_ | _ \n",
    ":---:|:---:\n",
    "<p style=\"text-align: left;\">+ DP-AGD outperforms / performs competitively <br>+ especially when budget is very small |<p style=\"text-align: left;\">+ SGD-Adv and SGD-MA depend on T (predetermined)<br>+ SGD-MA outperforms all others when budget > 0.8 (more splits due to tight bound on privacy loss)<br>--> impractical under strict privacy control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### SVM\n",
    "\n",
    "<img src=\"svm.png\" alt=\"sss1\" height=\"400\" width=\"700\" /> \n",
    "\n",
    "+ DP-AGD scores competitive accuracies\n",
    "    + shows unstable behavior on BANK (acc. decreases despite increased use of budget)\n",
    "    + Still, objective value consistently decreases as the budget is increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Results\n",
    "\n",
    "### KDDCup99\n",
    "\n",
    "![](kddcup99.png)\n",
    "\n",
    "+ Contains 5M samples, so mini-batch GD with 40,000-sample bathces is used\n",
    "+ OutPert is excluded becuase its sensitivity requires the full set\n",
    "+ DP-AGD outperforms or performs competitively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions: \n",
    "\n",
    "+ First known iterative optimization algorithm for differential privacy\n",
    "+ satisfies zCDP, which is weaker than (e)-DP, but stronger than (e,d)-DP\n",
    "+ Per-iteration privacy budget is adaptively determined during runtime based on utility of privacy-preserving statistics\n",
    "+ Uses gradient averaging to utilize non-descending gradients and preserve the budget\n",
    "+ Outperforms or performs competitively with baselines, with an advantage under strict privacy settings\n",
    "+ Code available: https://github.com/ppmlguy/DP-AGD\n",
    "+ Citation: \n",
    "```\n",
    "@inproceedings{Lee2018ConcentratedDP,\n",
    "  title={Concentrated Differentially Private Gradient Descent with Adaptive per-Iteration Privacy Budget},\n",
    "  author={Jaewoo Lee and Daniel Kifer},\n",
    "  booktitle={KDD},\n",
    "  year={2018}\n",
    "}\n",
    "```\n",
    "    + Lee, J., & Kifer, D. (2018). Concentrated Differentially Private Gradient Descent with Adaptive per-Iteration Privacy Budget. KDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
