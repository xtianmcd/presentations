{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\">Hyperparameter Optimization with hyperopt </p>\n",
    "\n",
    "### <p style=\"text-align: center;\"> Tree-Structured Parzen Estimation: An Expected Improvement algorithm </p>\n",
    "\n",
    "#### Based on [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) from Bergstra, _et. al._, published in NIPS 2011 Proceedings\n",
    "\n",
    "#### This [blog](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f) is great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\"> Machine Learning Algorithms Have Hyperparameters\n",
    "<br> <br>\n",
    "\n",
    "![](https://media1.tenor.com/images/40642ffe78c835655e495c841c2fb7bc/tenor.gif?itemid=5465236) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style=\"text-align: center;\"> Machine Learning Algorithms Have Hyperparameters\n",
    "\n",
    "## Which can leave you with hundreds of thousands to millions of combinations to search through...\n",
    "\n",
    "![](https://media.giphy.com/media/10PcMWwtZSYk2k/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "+ `hyperopt` uses Tree-Structured Parzen Estimation (TPE) to Model the hyperparameter search space\n",
    "+ As it tests new hyperparameter combinations, it updates what it knows about the search space to make increasingly informed choices \n",
    "\n",
    "<img src=\"TPE_performance.png\" alt=\"TPE_performance\" width=\"800\"/>\n",
    "\n",
    "#### [Library-Specific Implementations](https://github.com/hyperopt) * I have not tested these, except for hyperas: \n",
    "+ `hyperop-sklearn`: wrapper for machine learning library `scikit-learn`\n",
    "+ `hyperop-convnet` for `Theano`-based convolutional neural nets and `hyperopt-nnet` for feed-fwd neural nets\n",
    "+ `hyperas`: wrapper for the `keras` deep learning library; [Link](https://github.com/maxpumperla/hyperas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "+ `hyperopt` is easy to implement <br>\n",
    "\n",
    "+ You simply provide it with\n",
    "    + the **algorithm** being tested \n",
    "    + **objective function** to minimize\n",
    "    + the **parameters** to search\n",
    "    + the **ranges of values** for each parameter\n",
    "    + and the **initial distribution** of those ranges \n",
    "<br>\n",
    "+ The optimization algorithm updates those distributions as it runs to find the highest performing areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hyperopt is easy to implement!\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "    \n",
    "def objective(params):\n",
    "    \"\"\" algorithm with some loss function here \"\"\"\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "space = {\n",
    "        'epsilon'  : hp.choice('base_epsilon',\n",
    "                                [10**1,10**0,10**-1,10**-2,10**-3,10**-4]),\n",
    "        'momentum' : hp.quniform('initial_momentum', 0.0,0.9,0.1)\n",
    "        }\n",
    "            \n",
    "best = fmin(objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=Trials())\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <p style=\"text-align: center;\"> Tree-Structured Parzen Estimators : Sequential Model-Based Optimization (SMBO)</p>\n",
    "\n",
    "+ **Domain**: Search Space of all possible Hyperparameter combinations, represented in a Tree-Structure\n",
    "+ \n",
    "\n",
    "**Objective Function** | **Selection Criteria**\n",
    ":---:|:---:\n",
    "<img src=\"https://www.oreilly.com/library/view/machine-learning-with/9781787121515/assets/59dffc6c-561a-42a0-9a54-7a13b800ca7d.png\" alt=\"loss\" width=\"150\"/> <br> <img src=\"TPE_argmin.png\" alt=\"TPE_argmin\" width=\"100\"/> | <img src=\"TPE_selection.png\" alt=\"TPE_selection\" width=\"150\"/> <br> <img src=\"slxn_crit.png\" alt=\"slxn\" width=\"150\"/> \n",
    "\n",
    "+ **Surrogate Model**: $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$\n",
    "+ **History**: Keep track of the past and become increasingly \"less wrong\" with hyperparameter choices <p style=\"text-align: center;\"> <img src=\"TPE_model.png\" alt=\"TPE_model\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <p style=\"text-align: center;\"> Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
